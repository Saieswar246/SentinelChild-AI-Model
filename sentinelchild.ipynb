{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1a887d",
   "metadata": {},
   "source": [
    "This Jupyter notebook shows the \"SentinelChild Child Grooming Detector\" AI project.\n\"
    "Due to memory constraints, I'm providing the Google Drive link to download the dataset and models.\n\n"
    "**Dataset and models link:** https://drive.google.com/file/d/1dz5Tqr1P1obmanGfq84qelN5VqQBeRKk/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd2f42",
   "metadata": {},
   "source": [
    "Install the required modules #pip install numpy pandas scikit-learn matplotlib sentence-transformers joblib ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52714de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "                            roc_curve, roc_auc_score, precision_recall_curve, average_precision_score)\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67cdf9d",
   "metadata": {},
   "source": [
    "Create a directory to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd86da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"grooming_detection_models2\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3dfdb7",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73952073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load and preprocess the dataset\"\"\"\n",
    "    # Load predator IDs\n",
    "    with open(\"pan12-sexual-predator-identification-training-corpus-predators.txt\", \"r\") as f:\n",
    "        predator_ids = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    # Parse XML\n",
    "    tree = ET.parse(\"pan12-sexual-predator-identification-training-corpus.xml\")\n",
    "    root = tree.getroot()\n",
    "\n",
    "    data = []\n",
    "    for convo in root.findall(\"conversation\"):\n",
    "        convo_id = convo.get(\"id\")\n",
    "        authors = set()\n",
    "        full_text = []\n",
    "\n",
    "        for msg in convo.findall(\"message\"):\n",
    "            author = (msg.find(\"author\").text or \"\").strip()\n",
    "            text = (msg.find(\"text\").text or \"\").strip()\n",
    "            authors.add(author)\n",
    "            full_text.append(text)\n",
    "\n",
    "        label = int(any(author in predator_ids for author in authors))\n",
    "        data.append({\n",
    "            \"conversation_id\": convo_id,\n",
    "            \"text\": \" \".join(full_text),\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37043304",
   "metadata": {},
   "source": [
    "Model Training using SBERT and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df):\n",
    "    \"\"\"Train and evaluate the grooming detection model\"\"\"\n",
    "    # Generate embeddings\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = sbert_model.encode(df[\"text\"].tolist(), convert_to_tensor=False)\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        embeddings, y, stratify=y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "    \n",
    "    print(\"Running cross-validation...\")\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1: {np.mean(cv_scores):.2f} (Â±{np.std(cv_scores):.2f})\")\n",
    "\n",
    "    # Final training\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return sbert_model, clf, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f72a23",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aab5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(clf, X_test, y_test):\n",
    "    \"\"\"Comprehensive model evaluation with visualizations\"\"\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Non-Grooming\", \"Grooming\"]))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                display_labels=[\"Non-Grooming\", \"Grooming\"])\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    ap_score = average_precision_score(y_test, y_proba)\n",
    "    plt.plot(recall, precision, label=f'AP = {ap_score:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importance (for Logistic Regression)\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(20), clf.coef_[0][:20], align='center')\n",
    "        plt.yticks(range(20), [f\"Feature {i}\" for i in range(20)])\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dde2a",
   "metadata": {},
   "source": [
    "Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(sbert_model, clf):\n",
    "    \"\"\"Save all model components\"\"\"\n",
    "    print(f\"\\nSaving models to '{MODEL_DIR}'...\")\n",
    "    joblib.dump(clf, os.path.join(MODEL_DIR, \"classifier.pkl\"))\n",
    "    sbert_model.save(os.path.join(MODEL_DIR, \"sbert_model\"))\n",
    "    print(\"Models saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c1fcc",
   "metadata": {},
   "source": [
    "Prediction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0174ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroomingDetector:\n",
    "    def __init__(self, model_dir=MODEL_DIR):\n",
    "        \"\"\"Load saved models\"\"\"\n",
    "        self.sbert_model = SentenceTransformer(os.path.join(model_dir, \"sbert_model\"))\n",
    "        self.clf = joblib.load(os.path.join(model_dir, \"classifier.pkl\"))\n",
    "    \n",
    "    def predict(self, text, return_proba=False):\n",
    "        \"\"\"Make predictions on new text\"\"\"\n",
    "        embedding = self.sbert_model.encode([text])\n",
    "        proba = self.clf.predict_proba(embedding)[0][1]\n",
    "        \n",
    "        if return_proba:\n",
    "            return proba\n",
    "        return {\n",
    "            \"prediction\": \"Grooming\" if proba >= 0.5 else \"Non-Grooming\",\n",
    "            \"confidence\": float(proba),\n",
    "            \"is_grooming\": bool(proba >= 0.5)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d940e3",
   "metadata": {},
   "source": [
    "Main Execution and Sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting grooming detection pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n Loading data...\")\n",
    "    df = load_data()\n",
    "    print(f\"Loaded {len(df)} conversations\")\n",
    "    print(\"Class distribution:\")\n",
    "    display(df['label'].value_counts().to_frame())\n",
    "    \n",
    "    # 2. Train model\n",
    "    print(\"\\n Training model...\")\n",
    "    sbert_model, clf, X_test, y_test = train_model(df)\n",
    "    \n",
    "    # 3. Evaluate\n",
    "    print(\"\\n Evaluating model...\")\n",
    "    evaluate_model(clf, X_test, y_test)\n",
    "    \n",
    "    # 4. Save models\n",
    "    save_models(sbert_model, clf)\n",
    "    \n",
    "    # 5. Test predictions\n",
    "    print(\"\\n Testing predictions:\")\n",
    "    detector = GroomingDetector()\n",
    "    test_cases = [\n",
    "        \"Hey beautiful, wanna meet up?\",\n",
    "        \"The homework is due tomorrow\",\n",
    "        \"Don't tell your parents about us\",\n",
    "        \"You're so mature for your age\",\n",
    "        \"What's your favorite subject in school?\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_cases:\n",
    "        result = detector.predict(text)\n",
    "        print(f\"\\nText: {text[:60]}...\")\n",
    "        print(f\"Prediction: {result['prediction']} (Confidence: {result['confidence']:.2f})\")\n",
    "    \n",
    "    print(f\"\\n Total execution time: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = GroomingDetector()\n",
    "detector.predict(\"Don't tell your parents about us\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
